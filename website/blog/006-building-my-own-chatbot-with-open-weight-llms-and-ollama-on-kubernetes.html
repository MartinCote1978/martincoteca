<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Building My Own Chatbot with Open-Weight LLMs and Ollama on my own Kubernetes cluster</title>
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/5.1.3/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" rel="stylesheet">
  <link rel="stylesheet" href="../style.css">
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N5GN1K3DD0"></script>
  <script src="../analytics.js"></script>
</head>

<body>
  <div class="container">
    <div class="back-to-blogs">
      <a href="../blogs.html"><i class="fas fa-arrow-left"></i> Back to Blogs</a>
    </div>

    <article>
      <header class="blog-header">
        <div class="blog-date">November 15, 2025</div>
        <h1 class="blog-title">Building My Own Chatbot with Open-Weight LLMs and Ollama on my own Kubernetes cluster
        </h1>
        <div class="tags">
          <span class="tag">Kubernetes</span>
          <span class="tag">GCP</span>
          <span class="tag">Ollama</span>
          <span class="tag">LLM</span>
          <span class="tag">React</span>
          <span class="tag">Node.js</span>
          <span class="tag">Keycloak</span>
          <span class="tag">FluxCD</span>
          <span class="tag">GitOps</span>
          <span class="tag">Istio</span>
        </div>
      </header>

      <div class="blog-content">
        <h2>My own chatbot on my own Kubernetes cluster...</h2>
        <p>Over the past several months, I've embarked on a journey to build my own chatbot solution on my provious
          project,
          leveraging open-weight large language models (LLMs) and a modern cloud-native stack. This project has been
          both challenging and rewarding, teaching me valuable lessons about authentication, session management, GitOps
          automation, and the complexities of deploying AI workloads on Kubernetes.</p>

        <p>My chatbot solution integrates multiple open-weight models (Llama 3.2, Qwen3, and Mistral-Nemo)
          through <strong><a href="https://ollama.com" target="_blank">Ollama</a></strong>, a very great open source
          tool for running LLMs locally or in containers. The
          entire stack runs locally or on <strong>Google Cloud Platform (GCP)</strong> using <strong>Google Kubernetes
            Engine (GKE)</strong>, with infrastructure managed through <strong>Terraform</strong> and application
          deployments orchestrated via <strong><a href="https://fluxcd.io" target="_blank">FluxCD</a></strong> for true
          GitOps automation.
        </p>

        <h2>Overall Architecture and Setup</h2>

        <h3>The Technology Stack</h3>
        <p>The chatbot solution consists of several key components:</p>

        <ul>
          <li><strong>Frontend (React + Vite + shadcn-ui + Tailwind CSS + TypeScript)</strong>: A modern,
            monochrome-themed chatbot UI built with React, Vite, shadcn-ui, and Tailwind CSS. The UI supports multiple
            conversations, model selection, chat history management, and theme customization.</li>
          <li><strong>Backend API (Node.js + Express.js + Prisma + PostgreSQL)</strong>: A RESTful API server built on
            Node.js using Express.js, Prisma ORM, and PostgreSQL for managing users, chats, messages, and model
            configurations.</li>
          <li><strong>Ollama Service (Ollama + Helm chart)</strong>: Deployed as a Helm chart on Kubernetes, Ollama
            serves as the inference engine for multiple open-weight LLM models. Models are pre-loaded and managed
            through environment variables and persistent volumes.</li>
          <li><strong>Authentication Stack (Keycloak + OAuth2-proxy + Istio)</strong>: Keycloak provides identity and
            access management, with OAuth2-proxy handling session validation at the Istio gateway level.</li>
          <li><strong>Service Mesh (<a href="https://istio.io" target="_blank">Istio</a> + External-DNS + <a
                href="https://cert-manager.io" target="_blank">Cert-Manager</a>)</strong>: Istio manages ingress, mTLS,
            and
            traffic policies, integrating with OAuth2-proxy for authentication at the edge, External-DNS for DNS
            management, and Cert-Manager for TLS certificates.</li>
          <li><strong>GitOps Automation (FluxCD + Helm + <a href="https://kustomize.io"
                target="_blank">Kustomize</a>)</strong>: FluxCD continuously reconciles the desired
            state from Git, automatically deploying updates to the Kubernetes cluster.</li>
        </ul>

        <figure class="text-center">
          <img src="./006-chatbot-architecture.jpg" alt="Chatbot Architecture" class="img-fluid"
            style="width: 60%; height: auto;">
          <figcaption>High-level architecture of the chatbot platform — everything runs inside a single
            Kubernetes cluster with Istio-managed ingress, Keycloak-based authentication, and FluxCD-driven GitOps
            automation.</figcaption>
        </figure>

        <h3>Deployment Architecture</h3>
        <p>The entire solution is deployed on Docker-Desktop locally or on GKE using Terraform anda GitOps approach. The
          FluxCD Git repository contains Helm releases, Helm repositories, Image Policies & Image Automation (where
          necessary) and Kustomize configurations for each component, with environment-specific overlays for local
          development and sandbox environments. Key components include:</p>

        <pre><code>clusters/
├── base/           # Base configurations for both applications and infrastructure
├── components/     # Git submodules for each component
├── local/          # Local development environment
└── sandbox/        # Sandbox environment on GCP

components/
├── chatbot-ui/     # React frontend Helm chart
├── chatbot-api/    # Node.js backend API Helm chart
├── ollama/         # Ollama LLM inference service Helm chart
├── keycloak/       # Identity provider Helm chart
├── oauth2-proxy/   # Authentication proxy Helm chart
├── istio/          # Service mesh configuration
└── cert-manager/   # TLS certificate management Helm chart</code></pre>

        <p>Each component is versioned, containerized, and deployed through Helm charts stored in GitLab's container
          registry. FluxCD's image automation controller watches for new image tags and automatically updates Helm
          releases, ensuring deployments stay current with the latest builds.</p>

        <h3>Model Management</h3>
        <p>Ollama is configured to pull and run multiple models simultaneously. The available models are defined in the
          chatbot UI's environment variables as well as in the Ollama environment variables, for example:</p>

        <pre><code>VITE_AVAILABLE_MODELS:
  - id: "llama3.2:1b"
    name: "Llama 3.2"
    description: "Fast and efficient model"
  - id: "qwen3:0.6b"
    name: "Qwen3"
    description: "Latest generation of large language models"
  - id: "mistral-nemo:12b"
    name: "Mistral-Nemo"
    description: "Built by Mistral AI in collaboration with NVIDIA"</code></pre>

        <p>Users can switch between models dynamically through the UI, and the backend API tracks which model was used
          for each conversation, enabling model-specific analytics and optimization.</p>

        <h2>Key Challenges and Lessons Learned</h2>

        <h3>1. Authentication and Session Management: The Biggest Hurdle</h3>
        <p>One of the most significant challenges I faced was implementing authentication and session management.
          The integration between Keycloak, OAuth2-proxy, and the React frontend required careful orchestration of token
          refresh, callback handling, and state management. Something I was not familiar with and had to learn a lot
          about.</p>

        <h4>The Challenge: Token Refresh and PKCE</h4>
        <p>Implementing automatic token refresh was particularly tricky for me. The Keycloak JavaScript adapter provides
          token refresh capabilities, but coordinating this across React components, axios interceptors, and the
          authentication store required careful state management. I implemented a centralized auth store that manages
          token refresh logic:</p>

        <pre><code>// Token refresh in axios interceptor
axiosInstance.interceptors.response.use(
  async (error) => {
    if (error.response?.status === 401 && !error.config._retry) {
      error.config._retry = true;
      try {
        const newTokens = await authStore.refreshUserToken();
        if (newTokens?.access) {
          error.config.headers["Authorization"] = `Bearer ${newTokens.access}`;
          error.config.headers["x-auth-request-access-token"] = newTokens.access;
          return axiosInstance(error.config);
        }
      } catch (refreshError) {
        // Handle refresh failure - redirect to login
        return Promise.reject(refreshError);
      }
    }
    return Promise.reject(error);
  }
);</code></pre>

        <p>Implementing PKCE (Proof Key for Code Exchange) for enhanced security added further complexity. The Keycloak
          adapter needed to be configured with the correct PKCE method (S256) and response mode (fragment). Debugging
          PKCE issues required extensive logging and understanding of the OAuth2 flow.</p>

        <h4>The Challenge: CORS and Cross-Origin Token Propagation</h4>
        <p>Because the frontend (app.domain.com) and backend (api.domain.com) run on separate hosts, I had to explicitly
          manage every aspect of cross-origin access. The browser's CORS enforcement meant that the API had to expose
          precise headers — in my case Access-Control-Allow-Origin: app.domain.com, Access-Control-Allow-Credentials:
          true,
          and allowed HTTP verbs (GET, POST, PUT, PATCH, DELETE, OPTIONS) — to support preflight requests and session
          cookies. Without these, authentication would silently fail before even reaching the backend.</p>
        <p>The harder part was maintaining token continuity across Keycloak, OAuth2-proxy, and Istio. Once a user
          authenticated, the __domain_com cookie represented the session, while OAuth2-proxy validated it and injected
          the
          x-auth-request-access-token header into Istio's ext-authz flow. This ensured that every call from the frontend
          to api.domain.com carried a valid JWT without reauthentication. It took time to align the browser's CORS
          model,
          OAuth2 behavior, and Istio's security policies, but once configured, it provided seamless, end-to-end token
          propagation through the entire mesh.</p>

        <h4>The Challenge: Service Mesh Integration</h4>
        <p>Configuring Istio's external authorization to work with OAuth2-proxy required understanding both systems'
          authentication flows. The extension provider configuration needed to pass the correct headers upstream while
          validating sessions at the gateway level.</p>

        <h4>What I Learned</h4>
        <p>Building a good authentication system requires deep understanding of OAuth2 flows, token
          lifecycle management, and security best practices. The interaction between Keycloak, OAuth2-proxy, and the
          frontend application requires careful coordination of state, callbacks, and error handling. This was
          particularly challenging for me and remains so as I'm definitely not a security expert and had to learn a lot
          about OAuth2 and security best practices. Although it's working, I still don't think it is properly
          implemented following best practices.</p>
        <p>Troubleshooting authentication wasn't just about fixing one configuration—it required understanding how every
          piece in the chain interacted. A login failure could originate from a browser CORS policy, a missing redirect
          URI in Keycloak, an expired session in OAuth2-proxy, or a header not passed correctly through Istio's
          ext-authz layer. Learning to trace these flows step by step—from token issuance to validation—became essential
          to make the system stable.</p>
        <p>What made this challenging was that most tools work perfectly in isolation but fail subtly when combined.
          Keycloak expected OIDC semantics, Istio enforced mTLS and strict authorization, and the frontend obeyed
          browser security rules that none of the backend services were aware of.</p>

        <h3>2. GitOps and Infrastructure Automation</h3>
        <h4>The Challenge: FluxCD Submodule Recursion</h4>
        <p>FluxCD's bootstrap process removes recursive submodule configuration by default to avoid potential issues.
          However, my setup relies on Git submodules to reference component-specific repositories. I had to manually
          re-add the <code>recurseSubmodules: true</code> configuration after bootstrap, which wasn't immediately
          obvious for me and took me a while to figure out.</p>

        <h4>What I Learned</h4>
        <p>Adopting FluxCD for GitOps automation fundamentally changed how I manage deployments. Everything—from Helm
          releases to image updates—is now declarative, version-controlled, and continuously reconciled by the cluster
          itself. Instead of manually applying manifests or rebuilding environments, I simply commit changes to Git, and
          Flux ensures that the running state matches the desired one.</p>
        <p>The image automation workflow was especially powerful: whenever a new container image is built and pushed,
          Flux automatically updates the corresponding HelmRelease, triggering a clean, auditable rollout. This
          eliminated human error, made every deployment reproducible, and turned the cluster into a self-managing system
          that enforces configuration consistency by design.</p>
        <p>Having both a local (Docker Desktop) setup and a sandbox environment on GCP proved essential to developing
          with confidence. The local cluster gave me the freedom to iterate quickly, rebuild containers, and experiment
          with configuration changes without risk. The sandbox mirrored conditions with FluxCD GitOps
          automation, Istio routing, Keycloak authentication, and persistent storage, all running in the same topology
          as a real system.</p>
        <p>This dual-environment workflow made every deployment reproducible. I could validate Helm charts, Flux
          reconciliations, and Istio gateway rules exactly as they would behave in production before merging any change.
          It turned testing from trial-and-error into a disciplined feedback loop that consistently revealed integration
          issues early, when they were still easy to fix.</p>

        <h3>3. AI Workload Management</h3>
        <h4>The Challenge: Ollama Model Persistence</h4>
        <p>Ensuring Ollama models persist across pod restarts required proper volume configuration. I had to configure
          persistent volume claims (PVCs) with the correct access modes and ensure the Ollama data directory was
          properly mounted. Easy enough locally with Docker-Desktop, this was particular challenging on the Google Cloud
          Platform with GKE. The community Helm chart for Ollama required specific environment variables for model
          management.</p>

        <h4>What I Learned</h4>
        <p>Open-weight models like Llama 3.2 and Qwen3 provide excellent performance for my chatbot use cases. Ollama
          makes it straightforward to run multiple models simultaneously, allowing users to choose the best model for
          their specific needs. The models run efficiently on standard Kubernetes nodes with GPU support.</p>

        <h2>The End Result: A Full-featured Chatbot app running on Kubernetes</h2>
        <p>After months of development and iteration, the chatbot solution has evolved into a fully featured
          application. The user interface provides a clean, monochrome-themed experience that supports
          multiple authentication methods, dynamic model selection, and comprehensive user management. Below are
          screenshots showcasing the key features and pages of the application.</p>

        <h3>Authentication and Access</h3>
        <p>The login page supports multiple authentication methods, including email/password and Google OAuth (although
          removed / never fully implemented), providing users with flexible and secure access options.</p>
        <p>
          <img src="./006-chatbot-ui-login.jpg" alt="Chatbot UI - The login page, available with email or Google"
            class="img-fluid" style="width: 100%; height: auto;">
        </p>

        <h3>Core Chat Experience</h3>
        <p>The main chat interface allows users to select from multiple open-weight models, manage conversation history,
          and interact with the AI through a clean, intuitive interface, heavily inspired by ChatGPT.</p>
        <p>
          <img src="./006-chatbot-ui.jpg" alt="Chatbot UI - The chat page, with model selection and chat history"
            class="img-fluid" style="width: 100%; height: auto;">
        </p>
        <p>The chatbot supports advanced reasoning capabilities, in this example leveraging Qwen3, as shown in this
          example conversation where the model demonstrates step-by-step problem-solving for a very hard problem:
          answering "hi"!</p>
        <p>
          <img src="./006-chatbot-ui-chat-reasoning.jpg" alt="Chatbot UI - A chat example, with reasoning"
            class="img-fluid" style="width: 100%; height: auto;">
        </p>

        <h3>User Preferences and Management</h3>
        <p>Users can customize their experience through the settings page, which includes theme selection and language
          preferences (all implemented, thanks to an AI assistant).</p>
        <p>
          <img src="./006-chatbot-ui-settings.jpg"
            alt="Chatbot UI - The settings page, with theme selection and language setting" class="img-fluid"
            style="width: 100%; height: auto;">
        </p>
        <p>The account page provides users with access to their profile information and account management options.</p>
        <p>
          <img src="./006-chatbot-ui-account.jpg" alt="Chatbot UI - The account page, with user information"
            class="img-fluid" style="width: 100%; height: auto;">
        </p>
        <p>For future monetization, the plan page is ready to display subscription information and pricing tiers.</p>
        <p>
          <img src="./006-chatbot-ui-plan.jpg" alt="Chatbot UI - The plan page, with subscription information"
            class="img-fluid" style="width: 100%; height: auto;">
        </p>

        <h2>Conclusion</h2>
        <p>Building this chatbot solution has been an incredible learning experience. The combination of open-weight
          LLMs, modern cloud-native infrastructure, and GitOps automation creates a powerful, scalable platform for AI
          applications. While the journey involved significant challenges—particularly around authentication and session
          management—the end result is a production-ready system that demonstrates the maturity of open-source AI
          tooling.</p>

        <p>The solution showcases how modern DevOps practices (GitOps, Infrastructure as Code, containerization) can be
          applied to AI workloads, making them as manageable and scalable as traditional web applications, and has
          helped me learn a <strong>great deal</strong> about Kubernetes and its related technologies, GitOps and
          FluxCD, AI, LLMs and how they all work together.</p>

        <h2>Next Steps: Building My Own LLM</h2>
        <p>Looking forward, I'm planning to build my own LLM from scratch, using the knowledge I've gained from this
          project, focusing on both training and inference capabilities. I'm thinking to use <strong>nanoGPT</strong> as
          a reference implementation, which seems to provide an excellent foundation for understanding transformer
          architecture and training workflows.</p>

        <p>Once I have a trained model, I plan to integrate it into the existing Ollama-based infrastructure.</p>

        <p>The goal is to create a complete cycle: train a model, optimize it for inference, deploy it through the
          existing GitOps pipeline, and continuously improve it... It won't be as good as the big guys, but it will be
          a tremendous learning experience and it will be fun to build.</p>
      </div>
    </article>

    <div class="d-flex justify-content-center vertical-spacing">
      <a href="https://github.com/MartinCote1978" class="social-link" target="_blank">
        <i class="fab fa-github social-icon"></i>
      </a>
      <a href="https://www.linkedin.com/in/martincote" class="social-link" target="_blank">
        <i class="fab fa-linkedin-in social-icon"></i>
      </a>
      <a href="https://mastodon.online/@MartinCote" class="social-link" target="_blank">
        <i class="fab fa-mastodon social-icon"></i>
      </a>
      <a href="https://gitlab.com/bouc-io" class="social-link" target="_blank">
        <i class="fab fa-gitlab social-icon"></i>
      </a>
    </div>
  </div>

  <!-- Image Modal Overlay -->
  <div id="imageModal" class="image-modal">
    <span class="image-modal-close">&times;</span>
    <img class="image-modal-content" id="modalImage" alt="Expanded image">
  </div>

  <script src="https://stackpath.bootstrapcdn.com/bootstrap/5.1.3/js/bootstrap.min.js"></script>
  <script>
    // Dynamically load the social links block
    fetch('social-links.html')
      .then(response => response.text())
      .then(data => {
        document.getElementById('social-links').innerHTML = data;
      });

    // Image modal functionality
    (function () {
      const modal = document.getElementById('imageModal');
      const modalImg = document.getElementById('modalImage');
      const closeBtn = document.querySelector('.image-modal-close');
      const blogImages = document.querySelectorAll('.blog-content img');

      // Open modal when clicking on an image
      blogImages.forEach(function (img) {
        img.addEventListener('click', function () {
          modal.classList.add('active');
          modalImg.src = this.src;
          modalImg.alt = this.alt || 'Expanded image';
          document.body.style.overflow = 'hidden'; // Prevent background scrolling
        });
      });

      // Close modal when clicking the close button
      if (closeBtn) {
        closeBtn.addEventListener('click', function () {
          closeModal();
        });
      }

      // Close modal when clicking outside the image
      modal.addEventListener('click', function (e) {
        if (e.target === modal) {
          closeModal();
        }
      });

      // Close modal with Escape key
      document.addEventListener('keydown', function (e) {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
          closeModal();
        }
      });

      function closeModal() {
        modal.classList.remove('active');
        document.body.style.overflow = ''; // Restore scrolling
      }
    })();
  </script>
</body>

</html>