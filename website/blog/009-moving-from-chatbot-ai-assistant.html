<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Moving from Chatbot to AI Assistant</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/5.1.3/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../style.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-N5GN1K3DD0"></script>
    <script src="../analytics.js"></script>
</head>

<body>
    <div class="container">
        <div class="back-to-blogs">
            <a href="../blogs.html"><i class="fas fa-arrow-left"></i> Back to Blogs</a>
        </div>

        <article>
            <header class="blog-header">
                <div class="blog-date">February 17, 2026</div>
                <h1 class="blog-title">Moving from Chatbot to AI Assistant</h1>
                <div class="tags">
                    <span class="tag">Kubernetes</span>
                    <span class="tag">AI Assistant</span>
                    <span class="tag">Memory</span>
                    <span class="tag">Agent</span>
                    <span class="tag">LLM</span>
                </div>
            </header>

            <div class="blog-content">
                <p>Since building my <a
                        href="./006-building-my-own-chatbot-with-open-weight-llms-and-ollama-on-kubernetes.html">chatbot
                        solution</a>, I've been working on evolving it into a more capable AI assistant. The chatbot was
                    great for conversations, but I wanted to add capabilities that would make it more useful for complex
                    tasks and better at remembering context over time, trying to learn what those frontier labs are
                    doing and how they are doing it.</p>

                <h2>Adding Memory: Mostly Complete</h2>
                <p>The first major addition was a memory system that allows the assistant to remember important
                    information within conversations as well as across conversations (or assignments). This isn't just
                    chat history—it's a structured memory system that can store facts, preferences, and context that the
                    assistant can recall and use in future interactions.</p>
                <p>
                    <img src="./009-memory-ui.jpg" alt="Memory UI - The memory management interface" class="img-fluid"
                        style="width: 100%; height: auto;">
                </p>
                <p>The memory system is fully implemented and working. Users can view, search, and manage their stored
                    memories through a dedicated interface. The assistant can now remember things like user preferences,
                    important facts, and context from previous conversations, making interactions feel more personalized
                    and coherent over time. It can also technically retrieve via a vector search memories relevant to
                    the current conversation or assignment. It is however rudimentary, especially when I use one of the
                    small LLMs (e.g. Qwen3 3B) to determine the relevant memories.</p>
                <p>My first surprise was how this is all implemented! After doing some research and a LOT of use of
                    ChatGPT, Claude and Gemini, I basically learned that the overall process is simply to take the
                    entire chat or assignment and send it over to the LLM while giving it directive to extract and
                    segment, evaluate candidates and score those. The ones deemed relevant are then stored in the memory
                    system.
                </p>
                <p>This was so simple, I'm still unsure if that's really the way the frontier labs are doing it!! I was
                    directed to this research and will have to read it:
                <ul>
                    <li><a href="https://arxiv.org/abs/2310.08560" target="_blank">MemGPT: Towards LLMs as Operating
                            Systems</a></li>
                    <li><a href="https://arxiv.org/abs/2511.05495" target="_blank">IMDMR: An Intelligent
                            Multi-Dimensional Memory Retrieval System for Enhanced Conversational AI</a></li>
                </ul>
                </p>

                <h2>Adding Agent Capabilities: In Progress</h2>
                <p>The next step is adding agent capabilities—the ability for the assistant to break down complex tasks,
                    use tools, and execute multi-step workflows. This is still a work in progress, but I've started
                    building the foundation for it.</p>
                <p>
                    <img src="./009-agent-ui.jpg"
                        alt="Agent UI - The agent interface showing task planning and execution" class="img-fluid"
                        style="width: 100%; height: auto;">
                </p>
                <p>The agent system will allow the assistant to plan and execute more complex tasks, potentially using
                    external tools and APIs. Right now, the UI and basic structure are in place, but there's still work
                    to be done on the execution engine and tool integration.</p>
                <p>Again, I was directed to this research and will have to read it:
                <ul>
                    <li><a href="https://arxiv.org/abs/2210.03629" target="_blank">ReAct: Synergizing Reasoning and
                            Acting
                            in Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2302.04761" target="_blank">Toolformer: Language Models Can Teach
                            Themselves to Use Tools</a></li>
                </ul>
                </p>

                <h2>What's Next</h2>
                <p>I'm continuing to work on the agent capabilities, focusing on making it reliable and useful for
                    real-world tasks. The goal is to have an assistant that can not just chat, but actually help
                    accomplish complex workflows by breaking them down into steps and executing them systematically.</p>

                <section class="ai-disclosure mt-4 mb-4" style="font-size: 0.85rem; color: #adb5bd;">
                    <h2 class="h6 mb-2" style="font-size: 0.9rem;">AI Usage Disclosure</h2>
                    <p class="mb-0" style="font-size: 0.85rem;">This document was created with assistance from AI tools.
                        The content has been reviewed and edited by a human. For more information on the extent and
                        nature of AI usage, please contact the author.
                    </p>
                </section>
            </div>
        </article>

        <div class="d-flex justify-content-center vertical-spacing">
            <a href="https://github.com/MartinCote1978" class="social-link" target="_blank">
                <i class="fab fa-github social-icon"></i>
            </a>
            <a href="https://www.linkedin.com/in/martincote" class="social-link" target="_blank">
                <i class="fab fa-linkedin-in social-icon"></i>
            </a>
            <a href="https://mastodon.online/@MartinCote" class="social-link" target="_blank">
                <i class="fab fa-mastodon social-icon"></i>
            </a>
            <a href="https://gitlab.com/bouc-io" class="social-link" target="_blank">
                <i class="fab fa-gitlab social-icon"></i>
            </a>
        </div>
    </div>

    <!-- Image Modal Overlay -->
    <div id="imageModal" class="image-modal">
        <span class="image-modal-close">&times;</span>
        <img class="image-modal-content" id="modalImage" alt="Expanded image">
    </div>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/5.1.3/js/bootstrap.min.js"></script>
    <script>
        // Dynamically load the social links block
        fetch('social-links.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('social-links').innerHTML = data;
            });

        // Image modal functionality
        (function () {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const closeBtn = document.querySelector('.image-modal-close');
            const blogImages = document.querySelectorAll('.blog-content img');

            // Open modal when clicking on an image
            blogImages.forEach(function (img) {
                img.addEventListener('click', function () {
                    modal.classList.add('active');
                    modalImg.src = this.src;
                    modalImg.alt = this.alt || 'Expanded image';
                    document.body.style.overflow = 'hidden'; // Prevent background scrolling
                });
            });

            // Close modal when clicking the close button
            if (closeBtn) {
                closeBtn.addEventListener('click', function () {
                    closeModal();
                });
            }

            // Close modal when clicking outside the image
            modal.addEventListener('click', function (e) {
                if (e.target === modal) {
                    closeModal();
                }
            });

            // Close modal with Escape key
            document.addEventListener('keydown', function (e) {
                if (e.key === 'Escape' && modal.classList.contains('active')) {
                    closeModal();
                }
            });

            function closeModal() {
                modal.classList.remove('active');
                document.body.style.overflow = ''; // Restore scrolling
            }
        })();
    </script>
</body>

</html>